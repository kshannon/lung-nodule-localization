{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2D patches for training\n",
    "\n",
    "Let's run an end-to-end Keras training script with data from our S3 bucket. The data is stored on the S3 bucket in an HDF5 file. This test will give us an idea of the speed and cost of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/tf/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = !pwd\n",
    "# s3bucket_path = root_dir[0] + '/../s3bucket_goofys/' # remote S3 via goofys\n",
    "s3bucket_path = '/home/tony/Downloads/'\n",
    "path_to_hdf5 = s3bucket_path + 'LUNA16/hdf5-files/64x64x3-patch.hdf5'\n",
    "hdf5_file = h5py.File(path_to_hdf5, 'r') # open in read-only mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid hdf5 file in 'read' mode: <HDF5 file \"64x64x3-patch.hdf5\" (mode r)>\n",
      "Size of hdf5 file: 35.088 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid hdf5 file in 'read' mode: \" + str(hdf5_file))\n",
    "file_size = os.path.getsize(path_to_hdf5)\n",
    "print('Size of hdf5 file: {:.3f} GB'.format(file_size/2.0**30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 754962 images in the dataset.\n"
     ]
    }
   ],
   "source": [
    "num_rows = hdf5_file['input'].shape[0]\n",
    "print(\"There are {} images in the dataset.\".format(num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datasets within the HDF5 file are:\n",
      " [<HDF5 dataset \"centroid\": shape (754962, 3), type \"<f8\">, <HDF5 dataset \"input\": shape (754962, 12288), type \"<f4\">, <HDF5 dataset \"notrain\": shape (754962, 1), type \"<i8\">, <HDF5 dataset \"output\": shape (754962, 1), type \"<i8\">, <HDF5 dataset \"subsets\": shape (754962, 1), type \"<i8\">, <HDF5 dataset \"uuid\": shape (754962, 1), type \"|O\">]\n"
     ]
    }
   ],
   "source": [
    "print(\"The datasets within the HDF5 file are:\\n {}\".format(list(hdf5_file.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_idx(hdf5_file, classid = 0):\n",
    "    '''\n",
    "    Get the indices for the class classid and valid for training \n",
    "    '''\n",
    "#     # 1. Find indices from class classid\n",
    "#     idx_class = np.where( (hdf5_file['output'][:,0] == classid) )[0]\n",
    "    \n",
    "#     # 2. Find indices that are not excluded from training\n",
    "#     idx_notraining = np.where(hdf5_file[\"notrain\"][:,0] == 1)[0]\n",
    "    \n",
    "#     # 1. Find indices from class classid\n",
    "#     idx_class = np.where( (hdf5_file['output'][:,0] == classid) )[0]\n",
    "    \n",
    "#     # 2. Find indices that are not excluded from training\n",
    "#     idx_notraining = np.where(hdf5_file[\"notrain\"][:,0] == 1)[0]\n",
    "    \n",
    "     # 1. Find indices from class classid\n",
    "    idx_class = np.where( (hdf5_file['output'][:,0] == classid) )[0]\n",
    "    \n",
    "    # 2. Find indices that are not excluded from training\n",
    "    idx_notraining = np.where(hdf5_file[\"notrain\"][:,0] == 1)[0]\n",
    "                   \n",
    "    return np.setdiff1d(idx_class, idx_notraining) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_exclude_subset_idx(hdf5_file, idx, excluded_subset=0):\n",
    "    '''\n",
    "    Remove indices for the subset excluded_subset\n",
    "    '''   \n",
    "\n",
    "    excluded_idx = np.where(hdf5_file[\"subsets\"][:,0] == excluded_subset)[0] # indices\n",
    " \n",
    "    return np.setdiff1d(idx, excluded_idx)  # Remove the indices of the excluded subset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idx_for_classes(hdf5_file, excluded_subset=0):\n",
    "    '''\n",
    "    Get the indices for each class but don't include indices from excluded subset\n",
    "    '''\n",
    "    \n",
    "    idx = {}\n",
    "    idx[0] = get_class_idx(hdf5_file, 0)\n",
    "    idx[1] = get_class_idx(hdf5_file, 1)\n",
    "    \n",
    "    idx[0] = remove_exclude_subset_idx(hdf5_file, idx[0], excluded_subset)\n",
    "    idx[1] = remove_exclude_subset_idx(hdf5_file, idx[1], excluded_subset)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom HDF5 dataloader\n",
    "\n",
    "This is the first pass at our custom HDF5 data loader.\n",
    "We'll need to add data augmentation and class balancing to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_idx(hdf5_file, idx, batch_size = 20):\n",
    "    '''\n",
    "    Batch size needs to be even.\n",
    "    This is yield a balanced set of random indices for each class. \n",
    "    '''\n",
    "        \n",
    "    idx0 = idx[0]\n",
    "    idx1 = idx[1]\n",
    "    \n",
    "    # 2. Shuffle the two indices\n",
    "    np.random.shuffle(idx0)  # This shuffles in place\n",
    "    np.random.shuffle(idx1)  # This shuffles in place\n",
    "\n",
    "    # 3. Take half of the batch from each class\n",
    "    idx0_shuffle = idx0[0:(batch_size//2)]\n",
    "    idx1_shuffle = idx1[0:(batch_size//2)]\n",
    "\n",
    "    # Need to sort final list in order to slice\n",
    "    return np.sort(np.append(idx0_shuffle, idx1_shuffle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_rotate(img):\n",
    "    '''\n",
    "    Perform a random rotation on the tensor\n",
    "    `img` is the tensor\n",
    "    '''\n",
    "    shape = img.shape\n",
    "    # This will flip along n-1 axes. (If we flipped all n axes then we'd get the same result every time)\n",
    "    ax = np.random.choice(len(shape)-1,2, replace=False) # Choose randomly which axes to flip\n",
    "    return np.flip(img.swapaxes(ax[0], ax[1]), ax[0]) # Random +90 or -90 rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_flip(img):\n",
    "    '''\n",
    "    Performs a random flip on the tensor.\n",
    "    If the tensor is C x H x W x D this will perform flips on two of the C, H, D dimensions\n",
    "    If the tensor is C x H x W this will perform flip on either the H or the W dimension.\n",
    "    `img` is the tensor\n",
    "    '''\n",
    "    shape = img.shape\n",
    "    # This will flip along n-1 axes. (If we flipped all n axes then we'd get the same result every time)\n",
    "    ax = np.random.choice(len(shape)-1,len(shape)-2, replace=False) + 1 # Choose randomly which axes to flip\n",
    "    for i in ax:\n",
    "        img = np.flip(img, i) # Randomly flip along all but one axis\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_data(imgs):\n",
    "    ''' \n",
    "    Performs random flips, rotations, and other operations on the image tensors.\n",
    "    '''\n",
    "    \n",
    "    imgs_length = imgs.shape[0]\n",
    "    \n",
    "    for idx in range(imgs_length):\n",
    "        img = imgs[idx, :]\n",
    "        \n",
    "        if (np.random.rand() > 0.5):\n",
    "            img = img_flip(img)\n",
    "        \n",
    "#         if (np.random.rand() > 0.5):\n",
    "            \n",
    "#             if (np.random.rand() > 0.5):\n",
    "#                 img = img_rotate(img)\n",
    "\n",
    "#             if (np.random.rand() > 0.5):\n",
    "#                 img = img_flip(img)\n",
    "        \n",
    "#         else:\n",
    "            \n",
    "#             if (np.random.rand() > 0.5):\n",
    "#                 img = img_flip(img)\n",
    "                \n",
    "#             if (np.random.rand() > 0.5):\n",
    "#                 img = img_rotate(img)\n",
    "\n",
    "        imgs[idx,:] = img\n",
    "        \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(hdf5_file, batch_size=50, exclude_subset=0):\n",
    "    \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
    "    \"\"\" Randomly select batch_size rows from the hdf5 file dataset \"\"\"\n",
    "    \n",
    "    #input_shape = tuple([batch_size] + list(hdf5_file['input'].attrs['lshape']) + [1])\n",
    "    input_shape = (batch_size, 3,64,64,1)\n",
    "    \n",
    "    idx_master = get_idx_for_classes(hdf5_file, exclude_subset) \n",
    "        \n",
    "    random_idx = get_random_idx(hdf5_file, idx_master, batch_size)\n",
    "    imgs = hdf5_file[\"input\"][random_idx,:]\n",
    "    \n",
    "    imgs = imgs.reshape(input_shape)\n",
    "    imgs = np.swapaxes(imgs, 1,3)\n",
    "    ## Need to augment \n",
    "    #imgs = augment_data(imgs)\n",
    "\n",
    "    classes = hdf5_file[\"output\"][random_idx, 0] \n",
    "\n",
    "    return imgs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(hdf5_file, batch_size=50, exclude_subset=0):\n",
    "    \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
    "    \"\"\" Randomly select batch_size rows from the hdf5 file dataset \"\"\"\n",
    "    \n",
    "    input_shape = tuple([batch_size] + list(hdf5_file['input'].attrs['lshape']) + [1])\n",
    "    input_shape = (batch_size, 3,64,64,1)\n",
    "    \n",
    "    idx_master = get_idx_for_classes(hdf5_file, exclude_subset) \n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        random_idx = get_random_idx(hdf5_file, idx_master, batch_size)\n",
    "        imgs = hdf5_file[\"input\"][random_idx,:]\n",
    "        imgs = imgs.reshape(input_shape)\n",
    "        imgs = np.swapaxes(imgs, 1,3)\n",
    "        ## Need to augment \n",
    "        imgs = augment_data(imgs)\n",
    "        \n",
    "        classes = hdf5_file[\"output\"][random_idx, 0] \n",
    "        \n",
    "        yield imgs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = tuple(list(hdf5_file[\"input\"].attrs[\"lshape\"])) \n",
    "batch_size = 256   # Batch size to use\n",
    "print (input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tony/anaconda3/envs/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/tony/anaconda3/envs/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from resnet3d import Resnet3DBuilder\n",
    "model = Resnet3DBuilder.build_resnet_18((64, 64, 3, 1), 1)  # (input tensor shape, number of outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tony/anaconda3/envs/tf/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 3, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 32, 32, 2, 64 22016       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 2, 64 256         conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 2, 64 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 16, 16, 1, 64 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 16, 16, 1, 64 110656      max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 1, 64 256         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 1, 64 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 16, 16, 1, 64 110656      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 1, 64 0           max_pooling3d_1[0][0]            \n",
      "                                                                 conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 1, 64 256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 1, 64 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 16, 16, 1, 64 110656      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 1, 64 256         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 1, 64 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 16, 16, 1, 64 110656      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 1, 64 0           add_1[0][0]                      \n",
      "                                                                 conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 1, 64 256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 1, 64 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 8, 8, 1, 128) 221312      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 1, 128) 512         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 1, 128) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 8, 8, 1, 128) 8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 8, 8, 1, 128) 442496      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 1, 128) 0           conv3d_8[0][0]                   \n",
      "                                                                 conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 1, 128) 512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 1, 128) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 8, 8, 1, 128) 442496      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 8, 8, 1, 128) 512         conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 8, 8, 1, 128) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 8, 8, 1, 128) 442496      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 1, 128) 0           add_3[0][0]                      \n",
      "                                                                 conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1, 128) 512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 1, 128) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 4, 4, 1, 256) 884992      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 4, 4, 1, 256) 1024        conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 4, 4, 1, 256) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_13 (Conv3D)              (None, 4, 4, 1, 256) 33024       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_12 (Conv3D)              (None, 4, 4, 1, 256) 1769728     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 4, 4, 1, 256) 0           conv3d_13[0][0]                  \n",
      "                                                                 conv3d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 4, 4, 1, 256) 1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 4, 4, 1, 256) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_14 (Conv3D)              (None, 4, 4, 1, 256) 1769728     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 4, 4, 1, 256) 1024        conv3d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 4, 4, 1, 256) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_15 (Conv3D)              (None, 4, 4, 1, 256) 1769728     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4, 4, 1, 256) 0           add_5[0][0]                      \n",
      "                                                                 conv3d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 4, 4, 1, 256) 1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 4, 4, 1, 256) 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_16 (Conv3D)              (None, 2, 2, 1, 512) 3539456     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 2, 2, 1, 512) 2048        conv3d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 2, 2, 1, 512) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_18 (Conv3D)              (None, 2, 2, 1, 512) 131584      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_17 (Conv3D)              (None, 2, 2, 1, 512) 7078400     activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2, 2, 1, 512) 0           conv3d_18[0][0]                  \n",
      "                                                                 conv3d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 2, 2, 1, 512) 2048        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 2, 2, 1, 512) 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_19 (Conv3D)              (None, 2, 2, 1, 512) 7078400     activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 2, 2, 1, 512) 2048        conv3d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 2, 2, 1, 512) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_20 (Conv3D)              (None, 2, 2, 1, 512) 7078400     activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 2, 2, 1, 512) 0           add_7[0][0]                      \n",
      "                                                                 conv3d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 2, 2, 1, 512) 2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2, 2, 1, 512) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling3d_1 (AveragePoo (None, 1, 1, 1, 512) 0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling3d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,171,329\n",
      "Trainable params: 33,163,521\n",
      "Non-trainable params: 7,808\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tb_log = keras.callbacks.TensorBoard(log_dir='./tb_3D_logs', histogram_freq=0, batch_size=batch_size, \n",
    "                            write_graph=True, \n",
    "                            write_grads=True, write_images=True, \n",
    "                            embeddings_freq=0, embeddings_layer_names=None, \n",
    "                            embeddings_metadata=None)\n",
    "\n",
    "import time\n",
    "CHECKPOINT_FILENAME = \"cnn_3d_64_64_3\" + time.strftime(\"_%Y%m%d_%H%M%S\") + \"hdf5\"\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_FILENAME, verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with fit_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "   2/2949 [..............................] - ETA: 210:28:50 - loss: 1.4122 - acc: 0.6055"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generate_data(hdf5_file, batch_size, exclude_subset=2),\n",
    "                    steps_per_epoch=num_rows//batch_size, epochs=6,\n",
    "                    callbacks=[tb_log, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
