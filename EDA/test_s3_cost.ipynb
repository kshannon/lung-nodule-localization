{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test S3 for training\n",
    "\n",
    "Let's run an end-to-end Keras training script with data from our S3 bucket. The data is stored on the S3 bucket in an HDF5 file. This test will give us an idea of the speed and cost of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HDF5 on the S3 bucket for training in Keras\n",
    "\n",
    "This assumes you have [goofys](https://github.com/kahing/goofys) setup on your local machine.\n",
    "You'll probably first need to download and install the [AWS CLI](https://aws.amazon.com/cli/). If AWS CLI is properly installed then you should be able to run this command from your local Linux machine:\n",
    "\n",
    "` aws s3 ls s3://dse-cohort3-group5`\n",
    "\n",
    "If that works, then you can create a local directory with the command:\n",
    "\n",
    "`mkdir -p s3bucket`\n",
    "\n",
    "If that works, then you can use goofys to link the local directory with the s3 bucket.\n",
    "\n",
    "`./goofys dse-cohort3-group5 s3bucket`\n",
    "\n",
    "Once that is done, then you can access the s3bucket as if it were a local folder on your Linux machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3bucket_path = '/home/tony/Documents/Capstone/ucsd-dse-capstone/s3bucket/' # remote S3 via goofys\n",
    "#s3bucket_path = '/home/tony/Documents/Capstone/ucsd-dse-capstone/' # Local storage (for sanity test)\n",
    "path_to_hdf5 = s3bucket_path + 'LUNA16/hdf5-files/32dim_patches.hdf5'\n",
    "hdf5_file = h5py.File(path_to_hdf5, 'r') # open in read-only mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid hdf5 file in 'read' mode: <HDF5 file \"32dim_patches.hdf5\" (mode r)>\n",
      "Size of hdf5 file: 0.012 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid hdf5 file in 'read' mode: \" + str(hdf5_file))\n",
    "file_size = os.path.getsize(path_to_hdf5)\n",
    "print('Size of hdf5 file: {:.3f} GB'.format(file_size/2.0**30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom HDF5 dataloader\n",
    "\n",
    "This is the first pass at our custom HDF5 data loader.\n",
    "We'll need to add data augmentation and class balancing to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(hdf5_file, batch_size=50, num_rows=96, input_shape = (32,32,32,1)):\n",
    "    \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
    "    \"\"\" Randomly select batch_size rows from the hdf5 file dataset \"\"\"\n",
    "    \n",
    "    input_shape = tuple([batch_size] + list(input_shape))\n",
    "    while True:\n",
    "        \n",
    "        random_idx = np.sort(np.random.choice(num_rows, batch_size, replace=False))  \n",
    "        imgs = hdf5_file[\"patches\"][random_idx,:]\n",
    "        imgs = imgs.reshape(input_shape)\n",
    "        classes = hdf5_file[\"classes\"][random_idx,0]\n",
    "        yield imgs, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN\n",
    "\n",
    "This is a very simple 3D CNN just to test the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Images (InputLayer)          (None, 32, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 30, 30, 30, 96)    2688      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 15, 15, 96)    0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 324000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                10368032  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 10,370,857\n",
      "Trainable params: 10,370,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation,Conv3D,MaxPooling3D,Flatten,Dropout, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input_shape = (32,32,32,1)\n",
    "inputs = Input(input_shape, name='Images')\n",
    "\n",
    "conv1 = Conv3D(filters=96, kernel_size=(3, 3, 3), activation='relu', padding='valid',\n",
    "              kernel_initializer='glorot_uniform')(inputs)\n",
    "\n",
    "max2 = MaxPooling3D(pool_size=(2,2,2))(conv1)\n",
    "\n",
    "layer6 = Flatten()(max2)\n",
    "\n",
    "layer7 = Dense(32, activation='relu')(layer6)\n",
    "\n",
    "layer8 = Dropout(0.5)(layer7)\n",
    "\n",
    "layer9 = Dense(4, activation='relu')(layer8)\n",
    "\n",
    "layer10 = Dense(1, activation='sigmoid')(layer9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[layer10])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with fit_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2207/10000 [=====>........................] - ETA: 38:42 - loss: 0.0028 - acc: 0.9998"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history = model.fit_generator(generate_data(hdf5_file, batch_size, input_shape = (32,32,32,1)),\n",
    "                    steps_per_epoch=10000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
