{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test S3 for training\n",
    "\n",
    "Let's run an end-to-end Keras training script with data from our S3 bucket. The data is stored on the S3 bucket in an HDF5 file. This test will give us an idea of the speed and cost of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HDF5 on the S3 bucket for training in Keras\n",
    "\n",
    "This assumes you have [goofys](https://github.com/kahing/goofys) setup on your local machine.\n",
    "You'll probably first need to download and install the [AWS CLI](https://aws.amazon.com/cli/). If AWS CLI is properly installed then you should be able to run this command from your local Linux machine:\n",
    "\n",
    "` aws s3 ls s3://dse-cohort3-group5`\n",
    "\n",
    "If that works, then you can create a local directory with the command:\n",
    "\n",
    "`mkdir -p s3bucket`\n",
    "\n",
    "If that works, then you can use goofys to link the local directory with the s3 bucket.\n",
    "\n",
    "`./goofys dse-cohort3-group5 s3bucket`\n",
    "\n",
    "Once that is done, then you can access the s3bucket as if it were a local folder on your Linux machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3bucket_path = '/Users/aluthra/Documents/DSE/s3bucket/' # remote S3 via goofys\n",
    "#s3bucket_path = '/Users/aluthra/Documents/DSE/ucsd-dse-capstone/' # Local storage (for sanity test)\n",
    "path_to_hdf5 = s3bucket_path + 'LUNA16/hdf5-files/32dim_patches.hdf5'\n",
    "hdf5_file = h5py.File(path_to_hdf5, 'r') # open in read-only mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid hdf5 file in 'read' mode: <HDF5 file \"32dim_patches.hdf5\" (mode r)>\n",
      "Size of hdf5 file: 0.012 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid hdf5 file in 'read' mode: \" + str(hdf5_file))\n",
    "file_size = os.path.getsize(path_to_hdf5)\n",
    "print('Size of hdf5 file: {:.3f} GB'.format(file_size/2.0**30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom HDF5 dataloader\n",
    "\n",
    "This is the first pass at our custom HDF5 data loader.\n",
    "We'll need to add data augmentation and class balancing to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_idx(hdf5_file, batch_size = 32, exclude_subset=0):\n",
    "    '''\n",
    "    Batch size needs to be even.\n",
    "    This is yield a balanced set of random indices for each class. \n",
    "    '''\n",
    "    \n",
    "    # 1. Select indices from class 0 and 1\n",
    "    classes = hdf5_file[\"classes\"][:,0]\n",
    "    \n",
    "    idx0 = np.where(classes == 0)[0]  # Indices for class 0\n",
    "    idx1 = np.where(classes == 1)[0]  # Indices for class 1\n",
    "    #TODO -  Anil add try catch to check len(idx1) is greater than batch_size\n",
    "    #owing imbalance classes\n",
    "\n",
    "    subsets = hdf5_file[\"subset\"]\n",
    "    excluded_idx = np.where(subsets == exclude_subset)[0] # indices\n",
    "    idx0 = np.setdiff1d(idx0, excluded_idx)  # Remove the indices of the excluded subset\n",
    "    idx1 = np.setdiff1d(idx1, excluded_idx)  # Remove the indices of the excluded subset\n",
    "    \n",
    "    # 2. Shuffle the two indices\n",
    "    np.random.shuffle(idx0)  # This shuffles in place\n",
    "    np.random.shuffle(idx1)  # This shuffles in place\n",
    "\n",
    "    # 3. Take half of the batch from each class\n",
    "    idx0_shuffle = idx0[0:(batch_size//2)]\n",
    "    idx1_shuffle = idx1[0:(batch_size//2)]\n",
    "\n",
    "    # Need to sort final list in order to slice\n",
    "    return np.sort(np.append(idx0_shuffle, idx1_shuffle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = [32,32,32,1]\n",
    "\n",
    "ax = np.random.choice(len(shape)-1,2, replace=False) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36830395318360354"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_rotate(img):\n",
    "    '''\n",
    "    Perform a random rotation on the tensor\n",
    "    `img` is the tensor\n",
    "    '''\n",
    "    shape = img.shape\n",
    "    # This will flip along n-1 axes. (If we flipped all n axes then we'd get the same result every time)\n",
    "    ax = np.random.choice(len(shape)-1,2, replace=False) + 1 # Choose randomly which axes to flip\n",
    "    return np.flip(img.swapaxes(ax[0], ax[1]), ax[0]) # Random +90 or -90 rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_flip(img):\n",
    "    '''\n",
    "    Performs a random flip on the tensor.\n",
    "    If the tensor is C x H x W x D this will perform flips on two of the C, H, D dimensions\n",
    "    If the tensor is C x H x W this will perform flip on either the H or the W dimension.\n",
    "    `img` is the tensor\n",
    "    '''\n",
    "    shape = img.shape\n",
    "    # This will flip along n-1 axes. (If we flipped all n axes then we'd get the same result every time)\n",
    "    ax = np.random.choice(len(shape)-1,len(shape)-2, replace=False) + 1 # Choose randomly which axes to flip\n",
    "    for i in ax:\n",
    "        img = np.flip(img, i) # Randomly flip along all but one axis\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_data(imgs):\n",
    "    \n",
    "    imgs_length = imgs.shape[0]\n",
    "    \n",
    "    for idx in range(imgs_length):\n",
    "        img = imgs[idx, :]\n",
    "        \n",
    "        if (np.random.rand() > 0.5):\n",
    "            \n",
    "            if (np.random.rand() > 0.5):\n",
    "                img = img_rotate(img)\n",
    "\n",
    "            if (np.random.rand() > 0.5):\n",
    "                img = img_flip(img)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if (np.random.rand() > 0.5):\n",
    "            img = img_rotate(img)\n",
    "\n",
    "            if (np.random.rand() > 0.5):\n",
    "                img = img_flip(img)\n",
    "\n",
    "        imgs[idx,:] = img\n",
    "        \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(hdf5_file, batch_size=50, num_rows=96, input_shape = (32,32,32,1)):\n",
    "    \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
    "    \"\"\" Randomly select batch_size rows from the hdf5 file dataset \"\"\"\n",
    "    \n",
    "    input_shape = tuple([batch_size] + list(input_shape))\n",
    "    while True:\n",
    "        \n",
    "        random_idx = get_random_idx(hdf5_File, batch_size)\n",
    "        imgs = hdf5_file[\"patches\"][random_idx,:]\n",
    "        imgs = imgs.reshape(input_shape)\n",
    "        ## Need to augment \n",
    "        imgs = augment_data(imgs)\n",
    "        \n",
    "        classes = hdf5_file[\"classes\"][random_idx,0]\n",
    "        yield imgs, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D CNN\n",
    "\n",
    "This is a very simple 3D CNN just to test the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Images (InputLayer)          (None, 32, 32, 32, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 30, 30, 30, 96)    2688      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 15, 15, 96)    0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 324000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                10368032  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 10,370,857.0\n",
      "Trainable params: 10,370,857.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation,Conv3D,MaxPooling3D,Flatten,Dropout, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input_shape = (32,32,32,1)\n",
    "inputs = Input(input_shape, name='Images')\n",
    "\n",
    "conv1 = Conv3D(filters=96, kernel_size=(3, 3, 3), activation='relu', padding='valid',\n",
    "              kernel_initializer='glorot_uniform')(inputs)\n",
    "\n",
    "max2 = MaxPooling3D(pool_size=(2,2,2))(conv1)\n",
    "\n",
    "layer6 = Flatten()(max2)\n",
    "\n",
    "layer7 = Dense(32, activation='relu')(layer6)\n",
    "\n",
    "layer8 = Dropout(0.5)(layer7)\n",
    "\n",
    "layer9 = Dense(4, activation='relu')(layer8)\n",
    "\n",
    "layer10 = Dense(1, activation='sigmoid')(layer9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[layer10])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with fit_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2207/10000 [=====>........................] - ETA: 38:42 - loss: 0.0028 - acc: 0.9998"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "history = model.fit_generator(generate_data(hdf5_file, batch_size, input_shape = (32,32,32,1)),\n",
    "                    steps_per_epoch=10000, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
