{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read your HDF5 file! and Explore its contents.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "import h5py # Read the Docs: http://docs.h5py.org/en/latest/index.html\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_hdf5 = '/path/to/my.hdf5'\n",
    "hdf5_file = h5py.File(path_to_hdf5, 'r') # open in read-only mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid hdf5 file in 'read' mode: <HDF5 file \"64dim_patches.hdf5\" (mode r)>\n",
      "Size of the hdf5 file: 0.017 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Valid hdf5 file in 'read' mode: \" + str(hdf5_file))\n",
    "file_size = os.path.getsize(path_to_hdf5)\n",
    "print('Size of hdf5 file: {:.3f} GB'.format(file_size/2.0**30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info and some real data:\n",
      "classes\n",
      "<HDF5 dataset \"classes\": shape (365, 4), type \"<f8\">\n",
      "[[   0.           19.26483377   50.52493873 -176.063894  ]\n",
      " [   0.           74.34730878    5.43345505 -135.2220227 ]\n",
      " [   0.           15.98         22.08        -80.17      ]\n",
      " [   0.           79.78         24.89       -161.99      ]\n",
      " [   0.          -60.42953867  -57.0568771  -231.3005771 ]]\n",
      "patches\n",
      "<HDF5 dataset \"patches\": shape (365, 12288), type \"<f4\">\n",
      "[[ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]\n",
      " [ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "uuid\n",
      "<HDF5 dataset \"uuid\": shape (365, 1), type \"|O\">\n",
      "[[b'1.3.6.1.4.1.14519.5.2.1.6279.6001.281967919138248195763602360723']\n",
      " [b'1.3.6.1.4.1.14519.5.2.1.6279.6001.281967919138248195763602360723']\n",
      " [b'1.3.6.1.4.1.14519.5.2.1.6279.6001.281967919138248195763602360723']\n",
      " [b'1.3.6.1.4.1.14519.5.2.1.6279.6001.281967919138248195763602360723']\n",
      " [b'1.3.6.1.4.1.14519.5.2.1.6279.6001.281967919138248195763602360723']]\n"
     ]
    }
   ],
   "source": [
    "print('Dataset info and some real data:')\n",
    "for name in [key for key in dataset_name.keys()]:\n",
    "    print(name)\n",
    "    print(dataset_name[name]) #name + shape + dtype of the dataset (refer back to extract_patch.py)\n",
    "    print(dataset_name[name][0:5]) #get the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about ML? We need to Shuffle our dataset and prepare it for Keras. Alternatively for smaller datasets we can just pull all the data out and into numpy/pandas and use sklearn as usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None) #https://keras.io/utils/#hdf5matrix\n",
    "\n",
    "# with h5py.File('input/file.hdf5', 'r') as f:\n",
    "#     x_data = f['x_data']\n",
    "#     model.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the fit_generator method of your keras model. Just write your own generator class/function that pulls random batches of samples from your HDF5 file. That way, you never have to have all the data in memory at once. Similarly, if your validation data are too large to fit in memory, the validation_data argument to fit_generator also accepts a generator that produces batches from your validation data.\n",
    "\n",
    "# Essentially, you just need to do an np.random.shuffle on an array of indices into your data set, then split the random index array into training, validation, and testing array indices. Your generator arguments to fit_generator will just pull batches from your HDF5 file according to sequential batches of indices in the training and validation index arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
