{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To test: \n",
    "1. Create a folder ../data/luna16/\n",
    "2. Create a folder ../data/luna16/subset2\n",
    "    -Under this folder copy one scan for testing (script will process all the scan at this location) \n",
    "      1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405.mhd & raw file \n",
    "      (Google drive https://drive.google.com/drive/u/1/folders/13wmubTgm-7sh3MxPGxqmVZuoqi0G3ufW\n",
    "3. Create a folder ../data/luna16/hdf5\n",
    "    -Under this copy UNET_weights_H5.h5 (download from google drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "import os, glob \n",
    "import os, os.path\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from ipywidgets import interact\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from UNET_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = argparse.ArgumentParser(description='Prediction on HOLDOUT subset',add_help=True)\n",
    "# parser.add_argument(\"--holdout\", type=int, default=0, help=\"HOLDOUT subset for predictions\")\n",
    "# args = parser.parse_args()\n",
    "# HOLDOUT = args.holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOLDOUT = 5\n",
    "HO_dir = 'HO{}/'.format(HOLDOUT)\n",
    "data_dir = '../data/luna16/'\n",
    "model_wghts = 'hdf5/UNET_weights_H{}.h5'.format(HOLDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PADDED_SIZE = (448, 448, 368)\n",
    "SLICES = 8\n",
    "TILE_SIZE = (448,448,SLICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_create_loadWghts_Model_A(img_size=TILE_SIZE):\n",
    "    input_shape = tuple(list(img_size) + [1])\n",
    "    model = create_unet3D_Model_A(input_shape, use_upsampling=True)\n",
    "\n",
    "    model.load_weights(data_dir + model_wghts)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=[dice_coef_loss],\n",
    "                  metrics= [dice_coef])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_create_loadWghts(img_size=TILE_SIZE):\n",
    "    input_shape = tuple(list(img_size) + [1])\n",
    "    model = create_UNET3D(input_shape, use_upsampling=True)\n",
    "\n",
    "    model.load_weights(data_dir + model_wghts)\n",
    "#   ##Uncomment the followng line when just want to Transfer Weights to matching layers\n",
    "#     model.load_weights(data_dir + model_wghts, by_name=True)  \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'PredictionMask': dice_coef_loss, \\\n",
    "                        'PredictionClass': 'binary_crossentropy'}, \\\n",
    "                  loss_weights={'PredictionMask': 0.8, 'PredictionClass': 0.2},\n",
    "                  metrics={'PredictionMask':dice_coef,'PredictionClass': 'accuracy'})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mask(model, padded_img):\n",
    "    print ()\n",
    "    predicted_mask = np.zeros(PADDED_SIZE)\n",
    "    print (\"Total tiles : {}\".format(PADDED_SIZE[2]//SLICES))\n",
    "\n",
    "    for i in  tqdm(range( PADDED_SIZE[2]//SLICES), total=PADDED_SIZE[2]//SLICES, unit=\"tiles\"):\n",
    "#         print (\"Processing tile number : {}\".format(i))\n",
    "        tile = padded_img[:, :, (i*SLICES) : SLICES*(i+1)]\n",
    "        tile = tile.reshape(tuple([1] + list (tile.shape) + [1]))\n",
    "        tile_predictions = model.predict(tile, verbose=2)\n",
    "        \n",
    "        tile_mask = tile_predictions[0].reshape(TILE_SIZE)\n",
    "        predicted_mask[:, :, (i*SLICES) : SLICES*(i+1)] = tile_mask\n",
    "    return predicted_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing scan file: 1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405.mhd\n",
      "Original-Size of loaded image : (321, 512, 512)\n",
      "Normalized input image size: (285, 285, 321)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/46 [00:00<?, ?tiles/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded-image size: (448, 448, 368)\n",
      "\n",
      "Total tiles : 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/46 [02:08<1:36:02, 128.06s/tiles]\u001b[A\n",
      "  4%|▍         | 2/46 [04:15<1:33:46, 127.88s/tiles]\u001b[A\n",
      "  7%|▋         | 3/46 [06:20<1:30:48, 126.70s/tiles]\u001b[A\n",
      "  9%|▊         | 4/46 [08:21<1:27:49, 125.47s/tiles]\u001b[A\n",
      " 11%|█         | 5/46 [10:23<1:25:12, 124.69s/tiles]\u001b[A\n",
      " 13%|█▎        | 6/46 [12:28<1:23:11, 124.79s/tiles]\u001b[A\n",
      " 15%|█▌        | 7/46 [14:52<1:22:50, 127.46s/tiles]\u001b[A\n",
      " 17%|█▋        | 8/46 [17:09<1:21:30, 128.71s/tiles]\u001b[A\n",
      " 20%|█▉        | 9/46 [19:35<1:20:34, 130.66s/tiles]\u001b[A\n",
      " 22%|██▏       | 10/46 [21:39<1:17:59, 129.99s/tiles]\u001b[A\n",
      " 24%|██▍       | 11/46 [23:40<1:15:19, 129.12s/tiles]\u001b[A\n",
      " 26%|██▌       | 12/46 [25:47<1:13:04, 128.94s/tiles]\u001b[A\n",
      " 28%|██▊       | 13/46 [27:53<1:10:48, 128.73s/tiles]\u001b[A\n",
      " 30%|███       | 14/46 [30:01<1:08:37, 128.67s/tiles]\u001b[A\n",
      " 33%|███▎      | 15/46 [32:03<1:06:15, 128.24s/tiles]\u001b[A\n",
      " 35%|███▍      | 16/46 [34:04<1:03:53, 127.79s/tiles]\u001b[A\n",
      " 37%|███▋      | 17/46 [36:30<1:02:17, 128.88s/tiles]\u001b[A\n",
      " 39%|███▉      | 18/46 [38:54<1:00:30, 129.68s/tiles]\u001b[A\n",
      " 41%|████▏     | 19/46 [40:59<58:15, 129.46s/tiles]  \u001b[A\n",
      " 43%|████▎     | 20/46 [43:02<55:57, 129.14s/tiles]\u001b[A\n",
      " 46%|████▌     | 21/46 [45:03<53:38, 128.74s/tiles]\u001b[A\n",
      " 48%|████▊     | 22/46 [47:04<51:20, 128.37s/tiles]\u001b[A\n",
      " 50%|█████     | 23/46 [49:04<49:04, 128.00s/tiles]\u001b[A\n",
      " 52%|█████▏    | 24/46 [51:09<46:53, 127.88s/tiles]\u001b[A\n",
      " 54%|█████▍    | 25/46 [53:09<44:38, 127.56s/tiles]\u001b[A\n",
      " 57%|█████▋    | 26/46 [55:22<42:35, 127.78s/tiles]\u001b[A\n",
      " 59%|█████▊    | 27/46 [57:40<40:34, 128.15s/tiles]\u001b[A\n",
      " 61%|██████    | 28/46 [1:00:01<38:35, 128.61s/tiles]\u001b[A\n",
      " 63%|██████▎   | 29/46 [1:02:25<36:35, 129.17s/tiles]\u001b[A\n",
      " 65%|██████▌   | 30/46 [1:05:02<34:41, 130.09s/tiles]\u001b[A\n",
      " 67%|██████▋   | 31/46 [1:07:05<32:27, 129.86s/tiles]\u001b[A\n",
      " 70%|██████▉   | 32/46 [1:09:04<30:13, 129.52s/tiles]\u001b[A\n",
      " 72%|███████▏  | 33/46 [1:11:09<28:01, 129.38s/tiles]\u001b[A\n",
      " 74%|███████▍  | 34/46 [1:13:10<25:49, 129.12s/tiles]\u001b[A\n",
      " 76%|███████▌  | 35/46 [1:15:19<23:40, 129.13s/tiles]\u001b[A\n",
      " 78%|███████▊  | 36/46 [1:17:25<21:30, 129.03s/tiles]\u001b[A\n",
      " 80%|████████  | 37/46 [1:19:50<19:25, 129.47s/tiles]\u001b[A\n",
      " 83%|████████▎ | 38/46 [1:22:14<17:18, 129.86s/tiles]\u001b[A\n",
      " 85%|████████▍ | 39/46 [1:24:11<15:06, 129.51s/tiles]\u001b[A\n",
      " 87%|████████▋ | 40/46 [1:26:04<12:54, 129.10s/tiles]\u001b[A\n",
      " 89%|████████▉ | 41/46 [1:28:03<10:44, 128.86s/tiles]\u001b[A\n",
      " 91%|█████████▏| 42/46 [1:30:02<08:34, 128.64s/tiles]\u001b[A\n",
      " 93%|█████████▎| 43/46 [1:32:01<06:25, 128.40s/tiles]\u001b[A\n",
      " 96%|█████████▌| 44/46 [1:33:50<04:15, 127.96s/tiles]\u001b[A\n",
      " 98%|█████████▊| 45/46 [1:35:40<02:07, 127.57s/tiles]\u001b[A\n",
      "100%|██████████| 46/46 [1:37:31<00:00, 127.20s/tiles]\u001b[A\n",
      "100%|██████████| 1/1 [1:37:49<00:00, 5869.10s/files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Mask sum for entire scan: 0.0\n",
      "Processing runtime: 1:37:54.681545\n",
      "CPU times: user 3h 11min 11s, sys: 1h 16min 35s, total: 4h 27min 46s\n",
      "Wall time: 1h 37min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = datetime.now()\n",
    "predictions_dict = {}\n",
    "size_dict = {}\n",
    "model = model_create_loadWghts_Model_A(TILE_SIZE) \n",
    "fileCount = len(glob.glob(data_dir + 'subset2/' + '*.mhd'))\n",
    "                \n",
    "for f in tqdm(glob.glob(data_dir + 'subset2/' + '*.mhd'), total=fileCount, unit=\"files\") :\n",
    "    print (\"\\n Processing scan file: {}\".format(os.path.basename(f)))\n",
    "    seriesuid = os.path.splitext(os.path.basename(f))[0]\n",
    "    # Step-1\n",
    "    itk_img = sitk.ReadImage(f) \n",
    "    img_np_array = sitk.GetArrayFromImage(itk_img)\n",
    "    original_size = img_np_array.shape\n",
    "    print (\"Original-Size of loaded image : {}\".format(original_size))\n",
    "    # Step-2 \n",
    "    itk_img_norm = normalize_img(itk_img)\n",
    "    img_np_array_norm = sitk.GetArrayFromImage(itk_img_norm)\n",
    "    normalized_size = img_np_array_norm.shape\n",
    "    # Step-3 \n",
    "    img = img_np_array_norm.copy()\n",
    "#     img = normalize_HU(img_np_array_norm)\n",
    "    img = np.swapaxes(img, 0,2)   ##needed as SITK swaps axis  \n",
    "    print (\"Normalized input image size: {}\".format(img.shape))\n",
    "    # Step-4   # Step-5\n",
    "    padded_img = np.zeros(PADDED_SIZE)\n",
    "    padded_img[ :img.shape[0], :img.shape[1], :img.shape[2] ] = img\n",
    "    print (\"Padded-image size: {}\".format(padded_img.shape))\n",
    "    \n",
    "    predicted_mask = find_mask(model, padded_img)\n",
    "    predictions_dict[seriesuid] = (img.shape, padded_img, predicted_mask)\n",
    "    size_dict[seriesuid] = img.shape\n",
    "\n",
    "print('Predicted Mask sum for entire scan: {}'.format(np.sum(predicted_mask)))\n",
    "pickle.dump(predictions_dict, open('Model_A_noHU_entire_predictions_{}.dat'.format(seriesuid), 'wb'))\n",
    "pickle.dump(size_dict, open('Model_A_noHU_entire_size_{}.dat'.format(seriesuid), 'wb'))    \n",
    "print('Processing runtime: {}'.format(datetime.now() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820e0d08178a4035a1aee1718172841a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def displaySlice(sliceNo):\n",
    "    \n",
    "    plt.figure(figsize=[20,20]);    \n",
    "    plt.subplot(121)\n",
    "    plt.title(\"True Image\")\n",
    "    plt.imshow(padded_img[:, :, sliceNo], cmap='bone');\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(predicted_mask[:, :, sliceNo], cmap='bone');\n",
    "    plt.show()\n",
    "interact(displaySlice, sliceNo=(1,img.shape[2],1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Following sections for reference & WIP code snippets -AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Multiple tile test....performance hog, so exploiting the GPU for entire slice without compromising predictions \n",
    "##and for better performance  -AL\n",
    "\n",
    "# slices = 16\n",
    "# predicted_img = np.zeros(padded_size)\n",
    "\n",
    "# for i in range(368//slices):\n",
    "#     tile_1 = padded_img[:224, :224, (i*slices) : slices*(i+1)]\n",
    "#     tile_2 = padded_img[224:, 224:, (i*slices) : slices*(i+1) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slices = 8\n",
    "# predicted_mask = np.zeros(PADDED_SIZE)\n",
    "\n",
    "# for i in range(24//SLICES):\n",
    "#     tile = padded_img[:, :, (i*SLICES) : SLICES*(i+1)]\n",
    "#     tile = tile.reshape(tuple([1] + list (tile.shape) + [1]))\n",
    "# #     print(tile.shape)\n",
    "\n",
    "#     tile_predictions = model.predict(tile, verbose=2)\n",
    "#     tile_mask = tile_predictions[0].reshape(448, 448, 8)\n",
    "    \n",
    "#     print (tile_mask.shape)\n",
    "#     predicted_mask[:, :, (i*SLICES) : SLICES*(i+1)] = tile_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slices = 8\n",
    "# test_slice = padded_img[:, :, :slices]\n",
    "# print(test_slice.shape)\n",
    "# model = model_create_loadWghts(test_slice.shape) \n",
    "# # slice_predictions = model.predict(test_slice, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print (\"Shape of predicted mask or segmented image : {}\".format(predictions_small_img[0].shape))\n",
    "# print (\"Shape of predicted class : {}\".format(predictions_small_img[1].shape))\n",
    "# predictions_small_img[0] [:, 25 : 26, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## AL - TEST : making an image of size 48,48,48 with random 0 or 1\n",
    "# ### Case 2 : As a test created an input image of size (1, 48,48,48,1) \n",
    "# # with random 0 or 1; this works fine and able to create predictions successfully\n",
    "# t2 =  np.random.choice(2,(48,48,48))\n",
    "# t2 = t2.reshape(tuple([1] + list (t2.shape) + [1]))\n",
    "\n",
    "# print (\"Shape of test input image : {}\".format(t2.shape))\n",
    "# predictions = model.predict(t2, verbose=2)\n",
    "\n",
    "# print (\"Shape of predicted mask or segmented image : {}\".format(predictions[0].shape))\n",
    "# print (\"Shape of predicted class : {}\".format(predictions[1].shape))\n",
    "# # predictions[0] [:, 25 : 26, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padded_img[225:232, 225:232, 175]\n",
    "# predicted_mask[225:232, 225:232, 175]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
