{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To test: \n",
    "1. Create a folder ../data/luna16/\n",
    "2. Create a folder ../data/luna16/subset2\n",
    "    -Under this folder copy one scan for testing (script will process all the scan at this location) \n",
    "      1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016233746780170740405.mhd & raw file \n",
    "      (Google drive https://drive.google.com/drive/u/1/folders/13wmubTgm-7sh3MxPGxqmVZuoqi0G3ufW\n",
    "3. Create a folder ../data/luna16/hdf5\n",
    "    -Under this copy UNET_weights_H2.h5 (download from google drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import SimpleITK as sitk\n",
    "from PIL import Image\n",
    "import os, glob \n",
    "import os, os.path\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from ipywidgets import interact\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from UNET_utils import *\n",
    "from UNET_model_def import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = argparse.ArgumentParser(description='Prediction on HOLDOUT subset',add_help=True)\n",
    "# parser.add_argument(\"--holdout\", type=int, default=0, help=\"HOLDOUT subset for predictions\")\n",
    "# args = parser.parse_args()\n",
    "# HOLDOUT = args.holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOLDOUT = 5\n",
    "# HO_dir = 'HO{}/'.format(HOLDOUT)\n",
    "# data_dir = '/home/tony/data/luna16/'\n",
    "# model_wghts = 'hdf5/UNET_weights_modelB_H{}.h5'.format(HOLDOUT)\n",
    "\n",
    "TOP_DIR = \"/prj/qct/dl_engcomp/sandiego_scratch/capstone/dataset/hdffiles\"\n",
    "HO_dir = '/HO{}/'.format(HOLDOUT)\n",
    "data_dir = TOP_DIR + HO_dir\n",
    "\n",
    "\n",
    "model_wghts = \"{}/UNET_weights_modelB_exp1_lr009_BSexp1_H{}.h5\".format(data_dir, HOLDOUT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TILE_HSIZE = 64\n",
    "TILE_WSIZE = 64\n",
    "TILE_DSIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_create_loadWghts_Model_B():\n",
    "   \n",
    "    input_shape=(None, None, None, 1)\n",
    "    model = unet3D_Model6_Model13(input_shape, use_upsampling=True)\n",
    "\n",
    "    model.load_weights(model_wghts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_create_loadWghts_Model_A():\n",
    "   \n",
    "    model = unet3D_Model6_Model13((None,None,None,1))\n",
    "\n",
    "    model.load_weights(model_wghts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_create_loadWghts():\n",
    "    \n",
    "    input_shape=(None, None, None, 1)\n",
    "    model = create_UNET3D(input_shape, use_upsampling=True)\n",
    "\n",
    "    model.load_weights(data_dir + model_wghts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D U-Net Segmentation\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_Image (InputLayer)        (None, None, None, N 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1a (Conv3D)                 (None, None, None, N 896         Input_Image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, N 128         conv1a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, N 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1b (Conv3D)                 (None, None, None, N 55360       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, N 256         conv1b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, N 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling3D)            (None, None, None, N 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2a (Conv3D)                 (None, None, None, N 110656      pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, N 256         conv2a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, N 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2b (Conv3D)                 (None, None, None, N 221312      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, N 512         conv2b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, N 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling3D)            (None, None, None, N 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3a (Conv3D)                 (None, None, None, N 442496      pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, N 512         conv3a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, N 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, None, N 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3b (Conv3D)                 (None, None, None, N 884992      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, N 1024        conv3b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, N 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling3D)            (None, None, None, N 0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4a (Conv3D)                 (None, None, None, N 1769728     pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, N 1024        conv4a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, N 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, None, N 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv4b (Conv3D)                 (None, None, None, N 3539456     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, N 2048        conv4b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, N 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up4 (UpSampling3D)              (None, None, None, N 0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, N 0           up4[0][0]                        \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv5a (Conv3D)                 (None, None, None, N 5308672     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, N 1024        conv5a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, N 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5b (Conv3D)                 (None, None, None, N 1769728     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, N 1024        conv5b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, N 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up5 (UpSampling3D)              (None, None, None, N 0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, N 0           up5[0][0]                        \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6a (Conv3D)                 (None, None, None, N 1327232     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, N 512         conv6a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, N 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv6b (Conv3D)                 (None, None, None, N 442496      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, N 512         conv6b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, N 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up6 (UpSampling3D)              (None, None, None, N 0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, N 0           up6[0][0]                        \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv7a (Conv3D)                 (None, None, None, N 331840      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, N 256         conv7a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, N 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv7b (Conv3D)                 (None, None, None, N 55328       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, N 128         conv7b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, N 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "PredictionMask (Conv3D)         (None, None, None, N 33          activation_14[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 16,269,441\n",
      "Trainable params: 16,264,833\n",
      "Non-trainable params: 4,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_create_loadWghts_Model_B() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mask(model, img):\n",
    "    \n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    # Need to do ceiling to get the right prediction mask size\n",
    "    # This way prediction mask will be a multiple of the tile size.\n",
    "    # So just need to pad whole image to that multiple.\n",
    "    # At the end we crop back to the original image size.\n",
    "    pred_height = int(TILE_HSIZE * np.ceil(1.0*height/TILE_HSIZE))\n",
    "    pred_width = int(TILE_WSIZE * np.ceil(1.0*width/TILE_WSIZE))\n",
    "    pred_depth = int(TILE_DSIZE * np.ceil(1.0*depth/TILE_DSIZE))\n",
    "    \n",
    "    # Prediction mask is now a multiple of the TILE_SIZE.\n",
    "    prediction_mask = np.zeros((pred_height, pred_width, pred_depth, 1))\n",
    "    idxH = 0\n",
    "    \n",
    "    for startH_idx in range(0, height, TILE_HSIZE):\n",
    "        \n",
    "        stopH_idx = startH_idx + TILE_HSIZE\n",
    "        if stopH_idx > height:\n",
    "            stopH_idx = height\n",
    "            \n",
    "        idxW = 0\n",
    "        \n",
    "        for startW_idx in range(0, width, TILE_WSIZE):\n",
    "            \n",
    "            stopW_idx = startW_idx + TILE_WSIZE\n",
    "            if stopW_idx > width:\n",
    "                stopW_idx = width\n",
    "                \n",
    "            idxD = 0\n",
    "               \n",
    "            for startD_idx in range(0, depth, TILE_DSIZE):\n",
    "    \n",
    "                stopD_idx = startD_idx + TILE_DSIZE\n",
    "                if stopD_idx > depth:\n",
    "                    stopD_idx = depth\n",
    "            \n",
    "                snippet = img[startH_idx:stopH_idx, startW_idx:stopW_idx, startD_idx:stopD_idx]\n",
    "                \n",
    "                tile = np.zeros([TILE_HSIZE,TILE_WSIZE,TILE_DSIZE])\n",
    "                \n",
    "                snippet_width = stopW_idx - startW_idx\n",
    "                snippet_height = stopH_idx - startH_idx\n",
    "                snippet_depth = stopD_idx - startD_idx\n",
    "                \n",
    "                tile[:snippet_height,:snippet_width,:snippet_depth] = snippet\n",
    "                \n",
    "                tile = np.expand_dims(tile, 0)\n",
    "                tile = np.expand_dims(tile, -1)\n",
    "                print(tile.shape)\n",
    "\n",
    "                tile_mask = model.predict(tile, verbose=0)\n",
    "                \n",
    "                prediction_mask[idxH:(idxH+TILE_HSIZE), idxW:(idxW+TILE_WSIZE), idxD:(idxD+TILE_DSIZE), :] = tile_mask[0]\n",
    "                \n",
    "                idxD += TILE_DSIZE\n",
    "                \n",
    "            idxW += TILE_WSIZE\n",
    "            \n",
    "        idxH += TILE_HSIZE\n",
    "    \n",
    "    \n",
    "    return prediction_mask[:height, :width, :depth, :]  # Truncate to original image size\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D U-Net Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing scan file: 1.3.6.1.4.1.14519.5.2.1.6279.6001.112740418331256326754121315800.mhd\n",
      "Original-Size of loaded image : (149, 512, 512)\n",
      "Normalized input image size: (400, 400, 372)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n",
      "(1, 64, 64, 64, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c7f36233f7a7>\u001b[0m in \u001b[0;36mfind_mask\u001b[0;34m(model, img)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mtile_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mprediction_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTILE_HSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxW\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTILE_WSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxD\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTILE_DSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtile_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1800\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/prj/anaconda/anaconda3/anaconda3_gpu/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = datetime.now()\n",
    "predictions_dict = {}\n",
    "size_dict = {}\n",
    "model = model_create_loadWghts_Model_B() \n",
    "fileCount = len(glob.glob(data_dir + 'subset5/' + '*.mhd'))\n",
    "                \n",
    "for f in tqdm(glob.glob(data_dir + 'subset5/' + '*.mhd'), total=fileCount, unit=\"files\") :\n",
    "    print (\"\\n Processing scan file: {}\".format(os.path.basename(f)))\n",
    "    seriesuid = os.path.splitext(os.path.basename(f))[0]\n",
    "    # Step-1\n",
    "    itk_img = sitk.ReadImage(f) \n",
    "    img_np_array = sitk.GetArrayFromImage(itk_img)\n",
    "    original_size = img_np_array.shape\n",
    "    print (\"Original-Size of loaded image : {}\".format(original_size))\n",
    "    # Step-2 \n",
    "    itk_img_norm = normalize_img(itk_img)\n",
    "    img_np_array_norm = sitk.GetArrayFromImage(itk_img_norm)\n",
    "    normalized_size = img_np_array_norm.shape\n",
    "    # Step-3 \n",
    "    img = img_np_array_norm.copy()\n",
    "#     img = normalize_HU(img_np_array_norm)\n",
    "    img = np.swapaxes(img, 0,2)   ##needed as SITK swaps axis  \n",
    "    print (\"Normalized input image size: {}\".format(img.shape))\n",
    "    \n",
    "    predicted_mask = find_mask(model, img)\n",
    "    predictions_dict[seriesuid] = (img.shape, img, predicted_mask)\n",
    "    size_dict[seriesuid] = img.shape\n",
    "    \n",
    "\n",
    "print('Predicted Mask sum for entire scan: {}'.format(np.sum(predicted_mask)))\n",
    "pickle.dump(predictions_dict, open('Model_B_noHU_entire_predictions_{}.dat'.format(seriesuid), 'wb'))\n",
    "pickle.dump(size_dict, open('Model_B_noHU_entire_size_{}.dat'.format(seriesuid), 'wb'))    \n",
    "print('Processing runtime: {}'.format(datetime.now() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400, 372, 1)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400, 372)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f07d113e88641af9444239e331ade4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=186, description='sliceNo', max=372, min=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def displaySlice(sliceNo):\n",
    "    \n",
    "    plt.figure(figsize=[20,20]);    \n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(np.round(predicted_mask[:, sliceNo,:,0]), cmap='bone');\n",
    "    #plt.imshow(np.round(predicted_mask[100:350,100:350, sliceNo,0]), cmap='bone');\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Overlay Mask\")\n",
    "    plt.imshow(img[:, :, sliceNo].T, cmap=\"bone\");\n",
    "    #plt.imshow(img[sliceNo,:,:].T, cmap=\"bone\");\n",
    "    #plt.imshow(predicted_mask[:,:,sliceNo,0]>0.01, alpha=0.5, cmap='Reds');\n",
    "    plt.imshow(predicted_mask[:, sliceNo,:,0], alpha=0.5, cmap='Reds');\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "interact(displaySlice, sliceNo=(1,img.shape[2],1)); #172"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D U-Net Segmentation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing scan file: 1.3.6.1.4.1.14519.5.2.1.6279.6001.112740418331256326754121315800.mhd\n",
      "Original-Size of loaded image : (149, 512, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:12<00:00, 12.69s/files]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized input image size: (400, 400, 372)\n",
      "CPU times: user 1min 21s, sys: 4.83 s, total: 1min 26s\n",
      "Wall time: 16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = datetime.now()\n",
    "predictions_dict = {}\n",
    "size_dict = {}\n",
    "model = model_create_loadWghts_Model_B() \n",
    "fileCount = len(glob.glob(data_dir + 'subset5/' + '*.mhd'))\n",
    "                \n",
    "for f in tqdm(glob.glob(data_dir + 'subset5/' + '*.mhd'), total=fileCount, unit=\"files\") :\n",
    "    print (\"\\n Processing scan file: {}\".format(os.path.basename(f)))\n",
    "    seriesuid = os.path.splitext(os.path.basename(f))[0]\n",
    "    # Step-1\n",
    "    itk_img = sitk.ReadImage(f) \n",
    "    img_np_array = sitk.GetArrayFromImage(itk_img)\n",
    "    original_size = img_np_array.shape\n",
    "    print (\"Original-Size of loaded image : {}\".format(original_size))\n",
    "    # Step-2 \n",
    "    itk_img_norm = normalize_img(itk_img)\n",
    "    img_np_array_norm = sitk.GetArrayFromImage(itk_img_norm)\n",
    "    normalized_size = img_np_array_norm.shape\n",
    "    # Step-3 \n",
    "    img = img_np_array_norm.copy()\n",
    "#     img = normalize_HU(img_np_array_norm)\n",
    "    img = np.swapaxes(img, 0,2)   ##needed as SITK swaps axis  \n",
    "    print (\"Normalized input image size: {}\".format(img.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400, 1)\n"
     ]
    }
   ],
   "source": [
    "test_img = img[:, :, 281:282]\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400, 1, 1, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_img1 = np.expand_dims(np.expand_dims(test_img,-1),-1)\n",
    "# test_img1.shape\n",
    "\n",
    "# predict_array = model.predict(test_img1)\n",
    "\n",
    "# prediction_mask1 = np.round(predict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 400, 400, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "predicted_mask = find_mask(model, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_mask(model, img):\n",
    "    \n",
    "    height, width, depth = img.shape\n",
    "    TILE_HSIZE = height\n",
    "    TILE_WSIZE = width\n",
    "    TILE_DSIZE = depth\n",
    "    \n",
    "    # Need to do ceiling to get the right prediction mask size\n",
    "    # This way prediction mask will be a multiple of the tile size.\n",
    "    # So just need to pad whole image to that multiple.\n",
    "    # At the end we crop back to the original image size.\n",
    "    pred_height = int(TILE_HSIZE * np.ceil(1.0*height/TILE_HSIZE))\n",
    "    pred_width = int(TILE_WSIZE * np.ceil(1.0*width/TILE_WSIZE))\n",
    "    pred_depth = int(TILE_DSIZE * np.ceil(1.0*depth/TILE_DSIZE))\n",
    "    \n",
    "    # Prediction mask is now a multiple of the TILE_SIZE.\n",
    "    prediction_mask = np.zeros((pred_height, pred_width, pred_depth, 1))\n",
    "    idxH = 0\n",
    "    \n",
    "    for startH_idx in range(0, height, TILE_HSIZE):\n",
    "        \n",
    "        stopH_idx = startH_idx + TILE_HSIZE\n",
    "        if stopH_idx > height:\n",
    "            stopH_idx = height\n",
    "            \n",
    "        idxW = 0\n",
    "        \n",
    "        for startW_idx in range(0, width, TILE_WSIZE):\n",
    "            \n",
    "            stopW_idx = startW_idx + TILE_WSIZE\n",
    "            if stopW_idx > width:\n",
    "                stopW_idx = width\n",
    "                \n",
    "            idxD = 0\n",
    "               \n",
    "            for startD_idx in range(0, depth, TILE_DSIZE):\n",
    "    \n",
    "                stopD_idx = startD_idx + TILE_DSIZE\n",
    "                if stopD_idx > depth:\n",
    "                    stopD_idx = depth\n",
    "            \n",
    "                snippet = img[startH_idx:stopH_idx, startW_idx:stopW_idx, startD_idx:stopD_idx]\n",
    "                \n",
    "                tile = np.zeros([TILE_HSIZE,TILE_WSIZE,TILE_DSIZE])\n",
    "                \n",
    "                snippet_width = stopW_idx - startW_idx\n",
    "                snippet_height = stopH_idx - startH_idx\n",
    "                snippet_depth = stopD_idx - startD_idx\n",
    "                \n",
    "                tile[:snippet_height,:snippet_width,:snippet_depth] = snippet\n",
    "                \n",
    "                tile = np.expand_dims(tile, 0)\n",
    "                tile = np.expand_dims(tile, -1)\n",
    "                print(tile.shape)\n",
    "\n",
    "                tile_mask = model.predict(tile, verbose=0)\n",
    "                \n",
    "                prediction_mask[idxH:(idxH+TILE_HSIZE), idxW:(idxW+TILE_WSIZE), idxD:(idxD+TILE_DSIZE), :] = tile_mask[0]\n",
    "                \n",
    "                idxD += TILE_DSIZE\n",
    "                \n",
    "            idxW += TILE_WSIZE\n",
    "            \n",
    "        idxH += TILE_HSIZE\n",
    "    \n",
    "    \n",
    "    return prediction_mask[:height, :width, :depth, :]  # Truncate to original image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Following sections for reference & WIP code snippets -AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Multiple tile test....performance hog, so exploiting the GPU for entire slice without compromising predictions \n",
    "##and for better performance  -AL\n",
    "\n",
    "# slices = 16\n",
    "# predicted_img = np.zeros(padded_size)\n",
    "\n",
    "# for i in range(368//slices):\n",
    "#     tile_1 = padded_img[:224, :224, (i*slices) : slices*(i+1)]\n",
    "#     tile_2 = padded_img[224:, 224:, (i*slices) : slices*(i+1) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slices = 8\n",
    "# predicted_mask = np.zeros(PADDED_SIZE)\n",
    "\n",
    "# for i in range(24//SLICES):\n",
    "#     tile = padded_img[:, :, (i*SLICES) : SLICES*(i+1)]\n",
    "#     tile = tile.reshape(tuple([1] + list (tile.shape) + [1]))\n",
    "# #     print(tile.shape)\n",
    "\n",
    "#     tile_predictions = model.predict(tile, verbose=2)\n",
    "#     tile_mask = tile_predictions[0].reshape(448, 448, 8)\n",
    "    \n",
    "#     print (tile_mask.shape)\n",
    "#     predicted_mask[:, :, (i*SLICES) : SLICES*(i+1)] = tile_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# slices = 8\n",
    "# test_slice = padded_img[:, :, :slices]\n",
    "# print(test_slice.shape)\n",
    "# model = model_create_loadWghts(test_slice.shape) \n",
    "# # slice_predictions = model.predict(test_slice, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print (\"Shape of predicted mask or segmented image : {}\".format(predictions_small_img[0].shape))\n",
    "# print (\"Shape of predicted class : {}\".format(predictions_small_img[1].shape))\n",
    "# predictions_small_img[0] [:, 25 : 26, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## AL - TEST : making an image of size 48,48,48 with random 0 or 1\n",
    "# ### Case 2 : As a test created an input image of size (1, 48,48,48,1) \n",
    "# # with random 0 or 1; this works fine and able to create predictions successfully\n",
    "# t2 =  np.random.choice(2,(48,48,48))\n",
    "# t2 = t2.reshape(tuple([1] + list (t2.shape) + [1]))\n",
    "\n",
    "# print (\"Shape of test input image : {}\".format(t2.shape))\n",
    "# predictions = model.predict(t2, verbose=2)\n",
    "\n",
    "# print (\"Shape of predicted mask or segmented image : {}\".format(predictions[0].shape))\n",
    "# print (\"Shape of predicted class : {}\".format(predictions[1].shape))\n",
    "# # predictions[0] [:, 25 : 26, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padded_img[225:232, 225:232, 175]\n",
    "# predicted_mask[225:232, 225:232, 175]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
